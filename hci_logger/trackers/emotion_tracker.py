"""Emotion tracker usando DeepFace"""

import time
import cv2
from deepface import DeepFace
from pathlib import Path
from typing import Optional, Callable, Dict, Any
from threading import Thread, Event
import logging

logger = logging.getLogger(__name__)


class EmotionTracker:
    """Detecta emociones faciales usando DeepFace"""

    # 7 emociones b√°sicas que DeepFace detecta
    EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']

    def __init__(
        self,
        session_id: int,
        on_emotion_callback: Callable,
        sample_rate: float = 2.0,
        camera_id: int = 0,
        detector_backend: str = 'opencv',
        analyze_attributes: bool = True
    ):
        """
        Args:
            session_id: ID de la sesi√≥n actual
            on_emotion_callback: Callback llamado cuando se detecta emoci√≥n
            sample_rate: Frecuencia de an√°lisis en Hz (2.0 = cada 0.5s)
            camera_id: ID de la c√°mara (0 = default)
            detector_backend: Backend de detecci√≥n ('opencv', 'ssd', 'mtcnn', 'retinaface')
            analyze_attributes: Si analizar edad y g√©nero adem√°s de emociones
        """
        self.session_id = session_id
        self.on_emotion_callback = on_emotion_callback
        self.sample_rate = sample_rate
        self.frame_interval = 1.0 / sample_rate
        self.camera_id = camera_id
        self.detector_backend = detector_backend
        self.analyze_attributes = analyze_attributes

        self.cap: Optional[cv2.VideoCapture] = None
        self.running = False
        self.emotions_captured = 0

        # Thread y event
        self._thread: Optional[Thread] = None
        self._stop_event = Event()

        # Estado
        self.models_loaded = False
        self.last_detection_time = 0

    def start(self):
        """Iniciar detecci√≥n de emociones"""
        print(f"üòä Emotion tracker starting...")
        print(f"   Sample rate: {self.sample_rate} Hz")
        print(f"   Detector: {self.detector_backend}")
        print(f"   Analyze attributes: {self.analyze_attributes}")

        # Inicializar c√°mara
        self.cap = cv2.VideoCapture(self.camera_id)
        if not self.cap.isOpened():
            raise RuntimeError("‚ùå No se pudo abrir la c√°mara")

        # Configurar c√°mara
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

        print(f"   C√°mara inicializada: {int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x"
              f"{int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}")

        # Warm up DeepFace (descargar modelos en primera ejecuci√≥n)
        print(f"   Cargando modelos de DeepFace (puede tomar un momento)...")
        try:
            ret, frame = self.cap.read()
            if ret:
                # Primer an√°lisis para cargar modelos
                actions = ['emotion']
                if self.analyze_attributes:
                    actions.extend(['age', 'gender'])

                _ = DeepFace.analyze(
                    frame,
                    actions=actions,
                    detector_backend=self.detector_backend,
                    enforce_detection=False,
                    silent=True
                )
                self.models_loaded = True
                print(f"‚úì Modelos cargados exitosamente")
        except Exception as e:
            print(f"‚ö†Ô∏è  Advertencia durante warmup: {e}")

        self.running = True
        print(f"‚úì Emotion tracker started")

        # Iniciar thread de captura
        self._thread = Thread(
            target=self._capture_loop,
            daemon=True,
            name="EmotionTracker"
        )
        self._thread.start()

    def _capture_loop(self):
        """Loop principal de captura de emociones"""
        while self.running and not self._stop_event.is_set():
            try:
                # Control de sample rate
                current_time = time.time()
                time_since_last = current_time - self.last_detection_time

                if time_since_last < self.frame_interval:
                    time.sleep(self.frame_interval - time_since_last)
                    continue

                # Capturar frame
                ret, frame = self.cap.read()
                if not ret:
                    logger.warning("No se pudo capturar frame de c√°mara")
                    time.sleep(0.5)
                    continue

                # Analizar emociones
                result = self._analyze_frame(frame)

                if result:
                    # Llamar callback con resultado
                    self.on_emotion_callback(result)
                    self.emotions_captured += 1

                    # Log cada 10 detecciones
                    if self.emotions_captured % 10 == 0:
                        print(f"  üòä {self.emotions_captured} emociones detectadas")

                self.last_detection_time = time.time()

            except Exception as e:
                logger.error(f"Error en capture loop: {e}")
                time.sleep(1.0)  # Backoff en caso de error

    def _analyze_frame(self, frame) -> Optional[Dict[str, Any]]:
        """Analizar un frame para detectar emociones"""
        try:
            timestamp = time.time()

            # Configurar acciones a analizar
            actions = ['emotion']
            if self.analyze_attributes:
                actions.extend(['age', 'gender'])

            # Analizar con DeepFace
            results = DeepFace.analyze(
                frame,
                actions=actions,
                detector_backend=self.detector_backend,
                enforce_detection=False,  # No fallar si no hay cara
                silent=True
            )

            # DeepFace puede retornar lista o dict
            if not results:
                return None

            result = results[0] if isinstance(results, list) else results

            # Extraer emociones
            emotions = result.get('emotion', {})
            if not emotions:
                return None

            # Normalizar valores a 0-1
            emotion_data = {
                'session_id': self.session_id,
                'timestamp': timestamp,
                'angry': emotions.get('angry', 0) / 100.0,
                'disgust': emotions.get('disgust', 0) / 100.0,
                'fear': emotions.get('fear', 0) / 100.0,
                'happy': emotions.get('happy', 0) / 100.0,
                'sad': emotions.get('sad', 0) / 100.0,
                'surprise': emotions.get('surprise', 0) / 100.0,
                'neutral': emotions.get('neutral', 0) / 100.0,
                'dominant_emotion': result.get('dominant_emotion', 'unknown')
            }

            # A√±adir atributos opcionales
            if self.analyze_attributes:
                emotion_data['face_confidence'] = result.get('face_confidence', None)
                emotion_data['age'] = result.get('age', None)
                emotion_data['gender'] = result.get('dominant_gender',
                                                    result.get('gender', None))
            else:
                emotion_data['face_confidence'] = None
                emotion_data['age'] = None
                emotion_data['gender'] = None

            return emotion_data

        except Exception as e:
            logger.error(f"Error analizando frame: {e}")
            return None

    def stop(self, timeout: float = 5.0):
        """Detener detecci√≥n de emociones"""
        self.running = False
        self._stop_event.set()

        # Esperar thread
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=timeout)

        # Liberar c√°mara
        if self.cap:
            self.cap.release()
            self.cap = None

        print(f"‚úì Emotion tracker stopped ({self.emotions_captured} emotions captured)")

    def get_stats(self):
        """Obtener estad√≠sticas del tracker"""
        return {
            'emotions_captured': self.emotions_captured,
            'sample_rate': self.sample_rate,
            'detector_backend': self.detector_backend,
            'analyze_attributes': self.analyze_attributes,
            'running': self.running
        }


class EmotionTrackerAsync:
    """
    Wrapper async para EmotionTracker

    Proporciona interfaz consistente con otros trackers async
    """

    def __init__(
        self,
        session_id: int,
        on_emotion_callback: Callable,
        sample_rate: float = 2.0,
        camera_id: int = 0,
        detector_backend: str = 'opencv'
    ):
        self.tracker = EmotionTracker(
            session_id=session_id,
            on_emotion_callback=on_emotion_callback,
            sample_rate=sample_rate,
            camera_id=camera_id,
            detector_backend=detector_backend
        )

    def start(self):
        """Start emotion tracking"""
        self.tracker.start()

    def stop(self, timeout: float = 5.0):
        """Stop emotion tracking"""
        self.tracker.stop(timeout=timeout)

    def get_stats(self):
        """Get tracker stats"""
        return self.tracker.get_stats()


def test_emotion_detection():
    """Funci√≥n de test para verificar que DeepFace funciona"""
    print("üß™ Test de Emotion Detection\n")

    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("‚ùå No se pudo abrir la c√°mara")
        return

    print("üì∏ Capturando frame de prueba...")
    ret, frame = cap.read()
    cap.release()

    if not ret:
        print("‚ùå No se pudo capturar frame")
        return

    print("üîç Analizando con DeepFace...")
    try:
        result = DeepFace.analyze(
            frame,
            actions=['emotion', 'age', 'gender'],
            detector_backend='opencv',
            enforce_detection=False,
            silent=True
        )

        result = result[0] if isinstance(result, list) else result

        print("\n‚úÖ Detecci√≥n exitosa!\n")
        print("Emociones:")
        for emotion, value in result.get('emotion', {}).items():
            print(f"  {emotion}: {value:.1f}%")

        print(f"\nEmoci√≥n dominante: {result.get('dominant_emotion')}")
        print(f"Edad estimada: {result.get('age')}")
        print(f"G√©nero: {result.get('dominant_gender', result.get('gender'))}")

    except Exception as e:
        print(f"‚ùå Error: {e}")


if __name__ == "__main__":
    test_emotion_detection()
